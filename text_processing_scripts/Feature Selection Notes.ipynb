{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use intuition- which features do you think will have useful information?\n",
    "2. Code up feature\n",
    "3. Visualize feature - does this feature have discriminatory power?\n",
    "4. Repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BEWARE OF BUGS WHEN CREATING FEATURES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why get rid of features?\n",
    "\n",
    "- strongly related to another feature it is already present (multicollinearity)\n",
    "- it is noisy\n",
    "- feature causing overfitting\n",
    "- additional features slowing down train/test process\n",
    "\n",
    "##### Takeaway: features != information\n",
    "\n",
    "Want bare minimum of features that provide information. Get rid of features that do not provide much value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 important univariate feature selection tools in sklearn: \n",
    "\n",
    "- SelectPercentile: selects the X% of features that are most powerful (where X is a parameter)\n",
    "- SelectKBest: selects the K features that are most powerful (where K is a parameter).\n",
    "\n",
    "Text learning is a good candidate for feature selection, since the data has such high dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_classif,SelectPercentile,SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# f_classif is the anova f-value\n",
    "selector = SelectPercentile(f_classif, percentile=10) #get top 10 percent of features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfIdfVectorizer \n",
    "\n",
    "Key Terms:\n",
    "- <b/>bmax_df</b>: removes word that occur in greater frequency than threshold set here\n",
    "    - these words are considered corpus specific stopwords\n",
    "    \n",
    "    \n",
    "### Bias-Variance Dilema Revisted\n",
    "\n",
    "#### <b/> High Bias Algorithm: </b>\n",
    "    - pays little attention to data, oversimplified\n",
    "    - generalizes data\n",
    "    - does some thing over and over again\n",
    "    - high error on training set (low r^2, high SSE for regression)\n",
    " \n",
    "#### <b/> High Variance Algorithm: </b>\n",
    "    - pays too much attention to data, does not generalize it\n",
    "    - overfits\n",
    "    - memorizing training examples\n",
    "    - low error on training set (high r^2, low SSE for regression), high error on test set\n",
    "\n",
    "##### Example 1 : few features can lead into high-bias type regime\n",
    "\n",
    "You need several features, but only use 2 or 3, so not paying as much attention to data, oversimplifying things == high bias\n",
    " \n",
    "##### Example 2 : carefully minimized SSE\n",
    "\n",
    "Tradeoff between goodness of fit and simplicity of fit. Want to fit algorithm with optimal number of features\n",
    "\n",
    "### Regularization: \n",
    "\n",
    "#### Process of finding point at which quality of model is maximized, optimal trade-off between number of features and error\n",
    "\n",
    "##### Regularization in Regression: automatically penalizing extra features\n",
    "\n",
    "<b/> Lasso Regression </b>:\n",
    "    - in addition to minimize SSE, want to minimize features\n",
    "    - automatically takes into account penalty paramter so finds out which features have the highest importance\n",
    "    - weights features of little/no importance to 0, via changing coeffecient to 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lasso = linear_model.Lasso()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lasso.fit(x_train,y_train)\n",
    "pred = lasso.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
